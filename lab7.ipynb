{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "kgp-8HRNdmCP",
        "outputId": "27d2cd1a-8f74-4ab6-d5d5-3272976e955c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "MultiArmBandit() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-003ce199fc13>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Testing the epsilon_greedy strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mbandit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiArmBandit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_variance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MultiArmBandit() takes no arguments"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MultiArmBandit:\n",
        "    def _init_(self, arms, base_reward=5.0, reward_variance=0.5):\n",
        "        self.num_arms = arms\n",
        "        self.expected_rewards = np.array([base_reward] * self.num_arms, dtype=float)\n",
        "        self.reward_variance = reward_variance\n",
        "\n",
        "    def get_available_actions(self):\n",
        "        return list(range(self.num_arms))\n",
        "\n",
        "    def update_rewards(self, mean_increment=0.01):\n",
        "        increment = np.random.normal(0, mean_increment, self.num_arms)\n",
        "        self.expected_rewards += increment\n",
        "\n",
        "    def get_reward(self, action):\n",
        "        noise = np.random.normal(0, self.reward_variance)\n",
        "        return self.expected_rewards[action] + noise\n",
        "\n",
        "\n",
        "def epsilon_greedy(bandit, exploration_prob, iterations, dynamic_epsilon=False):\n",
        "    estimates = [0.0] * bandit.num_arms\n",
        "    action_counts = [0] * bandit.num_arms\n",
        "    instantaneous_rewards = []\n",
        "    average_rewards = [0.0]\n",
        "\n",
        "    for t in range(1, iterations + 1):\n",
        "        if dynamic_epsilon:\n",
        "            epsilon = exploration_prob / np.sqrt(t)  # Dynamically scale epsilon\n",
        "        else:\n",
        "            epsilon = exploration_prob\n",
        "\n",
        "        # Choose action\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(bandit.get_available_actions())  # Explore\n",
        "        else:\n",
        "            action = estimates.index(max(estimates))  # Exploit\n",
        "\n",
        "        # Get reward\n",
        "        reward = bandit.get_reward(action)\n",
        "        bandit.update_rewards()\n",
        "\n",
        "        # Update estimates\n",
        "        action_counts[action] += 1\n",
        "        estimates[action] += (reward - estimates[action]) / action_counts[action]\n",
        "\n",
        "        # Track rewards\n",
        "        instantaneous_rewards.append(reward)\n",
        "        average_rewards.append(average_rewards[-1] + (reward - average_rewards[-1]) / t)\n",
        "\n",
        "    return estimates, average_rewards, instantaneous_rewards\n",
        "\n",
        "\n",
        "# Testing the epsilon_greedy strategy\n",
        "bandit = MultiArmBandit(arms=3, base_reward=8.0, reward_variance=1.0)\n",
        "estimates, avg_rewards, inst_rewards = epsilon_greedy(bandit, exploration_prob=0.2, iterations=1500)\n",
        "\n",
        "# Plot 1: Average Rewards Accumulated Over Time\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(avg_rewards, label=\"Average Rewards Accumulated\", color='green')\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.title(\"Average Rewards Accumulated Over Time\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Instantaneous Rewards Given by Environment\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(inst_rewards, label=\"Instantaneous Rewards by Environment\", color='blue')\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Instantaneous Rewards Given by Environment\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Average Reward (Zoomed in on Accumulated Rewards)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(avg_rewards, label=\"Average Reward\", color='orange')\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Average Reward\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot 4: Instantaneous Rewards\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(inst_rewards, label=\"Instantaneous Rewards\", color='red')\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Instantaneous Rewards\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}